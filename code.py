# -*- coding: utf-8 -*-
"""Untitled6.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1DEYm0FEYXoWP4W54MArqjO_UC2tA7-HS
"""

from google.colab import drive
drive.mount('/content/drive')
data ='/content/drive/MyDrive/data'

import os, shutil, random
from sklearn.model_selection import train_test_split

random.seed(42)
src = data # Use the data variable defined in the previous cell
train_root = "dataset_train"

val_root = "dataset_val"
os.makedirs(train_root, exist_ok=True)
os.makedirs(val_root, exist_ok=True)

for person in os.listdir(src):
    person_dir = os.path.join(src, person)
    if not os.path.isdir(person_dir):
        continue
    imgs = [f for f in os.listdir(person_dir) if f.lower().endswith(('.jpg','.jpeg','.png'))]
    if not imgs:
        continue
    train_files, val_files = train_test_split(imgs, test_size=0.2, random_state=42)
    os.makedirs(os.path.join(train_root, person), exist_ok=True)
    os.makedirs(os.path.join(val_root, person), exist_ok=True)
    for f in train_files:
        shutil.copy(os.path.join(person_dir, f), os.path.join(train_root, person, f))
    for f in val_files:
        shutil.copy(os.path.join(person_dir, f), os.path.join(val_root, person, f))
print("Split done.")

#pip install facenet-pytorch

from facenet_pytorch import MTCNN
from PIL import Image

mtcnn = MTCNN(keep_all=False)  # single face

def crop_face(pil_image):
    # returns a PIL cropped face or None if not found
    try:
        face = mtcnn(pil_image)
        # mtcnn returns a torch tensor if successful (C, H, W)
        if face is None:
            return None
        # convert to PIL
        face_pil = Image.fromarray((face.permute(1,2,0).numpy()*255).astype('uint8'))
        return face_pil
    except Exception:
        return None

from facenet_pytorch import InceptionResnetV1
from PIL import Image
import torch
import numpy as np

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Load pretrained FaceNet model
model = InceptionResnetV1(pretrained='vggface2').eval().to(device)

def image_to_embedding(pil_img):
    # Convert PIL image to torch tensor and normalize
    img_tensor = torch.tensor(np.array(pil_img)).permute(2,0,1).unsqueeze(0).float() / 255.0
    # FaceNet expects inputs in [-1,1]
    img_tensor = (img_tensor - 0.5) * 2
    img_tensor = img_tensor.to(device)

    with torch.no_grad():
        emb = model(img_tensor)
    emb = emb.cpu().numpy().squeeze()
    emb = emb / np.linalg.norm(emb)  # L2 normalize
    return emb.astype('float32')

import os, json
from tqdm import tqdm
import numpy as np
from PIL import Image

train_root = "dataset_train"
embs = []
labels = []

for person in os.listdir(train_root):
    pdir = os.path.join(train_root, person)
    if not os.path.isdir(pdir):
        continue
    for fn in os.listdir(pdir):
        if not fn.lower().endswith(('.jpg','.png','.jpeg')):
            continue
        path = os.path.join(pdir, fn)
        pil = Image.open(path).convert("RGB")
        face = crop_face(pil)  # recommended
        if face is None:
            continue  # skip images without detected face
        emb = image_to_embedding(face)
        embs.append(emb)
        labels.append(person)

embs = np.stack(embs)  # shape (N, D)
np.save("embs.npy", embs)
with open("meta.json", "w") as f:
    json.dump(labels, f)

print("Saved embeddings:", embs.shape)

import faiss
import numpy as np
embs = np.load("embs.npy").astype('float32')
index = faiss.IndexFlatIP(embs.shape[1])  # inner product
index.add(embs)  # add all embedings
faiss.write_index(index, "faiss_index.index")
print("FAISS index created with", index.ntotal, "vectors")

import numpy as np, json
from sklearn.metrics.pairwise import cosine_similarity
from tqdm import tqdm
from PIL import Image

val_root = "dataset_val"
embs = np.load("embs.npy")
labels = json.load(open("meta.json"))

# helper: compute best similarity of emb to each training person's embeddings
# for speed, create per-person grouped embeddings
from collections import defaultdict
person_embs = defaultdict(list)
for e, name in zip(embs, labels):
    person_embs[name].append(e)
for k in person_embs:
    person_embs[k] = np.stack(person_embs[k])

val_scores = []  # list of tuples: (true_name, best_name, best_score)
for person in os.listdir(val_root):
    pdir = os.path.join(val_root, person)
    if not os.path.isdir(pdir):
        continue
    for fn in os.listdir(pdir):
        if not fn.lower().endswith(('.jpg','.png','.jpeg')):
            continue
        pil = Image.open(os.path.join(pdir, fn)).convert("RGB")
        face = crop_face(pil)
        if face is None:
            continue
        emb = image_to_embedding(face)  # normalized
        best_name, best_score = "Unknown", -1.0
        for celeb, embs_arr in person_embs.items():
            # compare to all stored images of this celeb, take max
            sim = np.dot(emb, embs_arr.T).max()
            if sim > best_score:
                best_score = float(sim)
                best_name = celeb
        val_scores.append((person, best_name, best_score))

# find best threshold
best_t, best_acc = 0.0, 0.0
for t in np.linspace(0.2, 0.95, 76):  # sweep candidate thresholds
    correct = 0
    for true, pred_name, score in val_scores:
        if score >= t and pred_name == true:
            correct += 1
    acc = correct / len(val_scores)
    if acc > best_acc:
        best_acc = acc
        best_t = t

print(f"Best threshold {best_t:.3f} with validation accuracy {best_acc:.3f}")

import gradio as gr

def infer_gradio(img):
    # img is a numpy array from gradio; convert to PIL
    pil = Image.fromarray(img.astype('uint8'), 'RGB')
    face = crop_face(pil)
    if face is None:
        return "No face found", None
    out = identify_image_from_pil(face)  # you can write a small wrapper similar to identify_image
    return f"{out['result']} â€” {out['name']} (score={out['score']:.3f})"

demo = gr.Interface(fn=infer_gradio, inputs=gr.Image(type="numpy"), outputs="text", title="Face access")
demo.launch()



!pip install faiss-cpu

